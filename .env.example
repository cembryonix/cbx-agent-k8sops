# LLM Provider Configuration
LLM_PROVIDER=anthropic  # anthropic | openai | ollama
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
OLLAMA_BASE_URL=http://localhost:11434

# Default model
MODEL_NAME=claude-sonnet-4-20250514

# MCP Server Configuration
MCP_TRANSPORT=http  # stdio | http
MCP_SERVER_URL=http://localhost:8080/mcp

# For stdio transport (local development)
# MCP_TRANSPORT=stdio
# MCP_SERVER_COMMAND=python
# MCP_SERVER_ARGS=./mcp_server/main.py

# Application Settings
DEBUG=true
