# K8SOps Agent - Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# LLM Provider - choose one and set its API key
# =============================================================================

# Options: anthropic, openai, ollama
LLM_PROVIDER=anthropic

# Anthropic
ANTHROPIC_API_KEY=sk-ant-api03-xxxxx

# OpenAI (uncomment if using)
#OPENAI_API_KEY=sk-xxxxx

# Ollama (uncomment if using, no API key needed)
#OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# Default LLM Settings
# =============================================================================

MODEL_NAME=claude-sonnet-4-5-20250929

# =============================================================================
# MCP Server Connection
# =============================================================================

MCP_TRANSPORT=http
MCP_SERVER_URL=http://localhost:8080/mcp
MCP_SSL_VERIFY=false

# For stdio transport (local development)
#MCP_TRANSPORT=stdio
#MCP_SERVER_COMMAND=python
#MCP_SERVER_ARGS=./mcp_server/main.py

# =============================================================================
# Agent Memory - Storage Backend
# =============================================================================

# Memory backend type: memory, filesystem, or redis
# - memory: In-memory only, no persistence (default, for testing)
# - filesystem: Local JSONL files (for local development)
# - redis: Redis server (for production)
MEMORY_BACKEND=filesystem

# Filesystem path (used when MEMORY_BACKEND=filesystem)
# Default: ~/.k8sops
#MEMORY_FILESYSTEM_PATH=~/.k8sops

# Redis URL (required when MEMORY_BACKEND=redis)
#REDIS_URL=redis://localhost:6379

# Use shallow checkpointer (only stores latest state, not full history)
# Recommended for production to reduce memory usage
#MEMORY_SHALLOW=true

# =============================================================================
# Agent Memory - Long-term (optional, requires Redis)
# =============================================================================

# Enable long-term memory (learns from sessions, retrieves relevant context)
#MEMORY_LONG_TERM_ENABLED=true

# Embedding provider for semantic search
# Options: openai, ollama (Anthropic has no embeddings API)
#EMBEDDING_PROVIDER=openai

# Embedding model (defaults: openai=text-embedding-3-small, ollama=nomic-embed-text)
#EMBEDDING_MODEL=text-embedding-3-small

# Context window management
# Threshold (0.0-1.0) that triggers conversation summarization
#MEMORY_CONTEXT_THRESHOLD=0.75

# Maximum tokens before triggering summarization
#MEMORY_MAX_CONTEXT_TOKENS=100000

# Number of memories to retrieve per search
#MEMORY_MAX_MEMORIES=5

# =============================================================================
# Session Management
# =============================================================================

# Maximum number of chat sessions to keep per user
#MAX_SESSIONS=10

# User ID for session/memory namespace (use default when no auth)
#MEMORY_USER_ID=default

# =============================================================================
# Application Settings (optional)
# =============================================================================

# Options: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

DEBUG=false