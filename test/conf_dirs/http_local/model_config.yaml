global_config:
 temperature: 0.1
 max_tokens: 2000
 timeout: 30
 max_retries: 3

providers:
 openai:
   models:
     gpt-4.1-nano:
       config_overrides: {}

     gpt-4o-mini:
       context_window: 128000
       config_overrides: {}
     gpt-4o:
       config_overrides:
         temperature: 0.1
     gpt-5-nano:
       config_overrides:
         reasoning_effort: minimal
         verbosity: low
     gpt-5-mini:
       context_window: 272000
       config_overrides:
         streaming: false
         reasoning_effort: minimal
         verbosity: low
     gpt-5:
       config_overrides:
         streaming: false
         reasoning_effort: minimal
         verbosity: low
 ollama:
   base_config:
     base_url: http://localhost:11434
   models:
     llama3.2:
       context_window: 4096
       config_overrides:
         temperature: 0.8

# anthropic:
#   models:
#     claude-3-5-sonnet-20241022:
#       config_overrides:
#         max_tokens: 4096